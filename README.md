# ğŸ§  Arabizi Synthetic Dataset Generator

A pipeline to generate **Lebanese Arabizi** (Arabic in Latin script, e.g., "kifak", "mni7", "shou") from English conversational data using:

âœ… GPT-based phonetic & cultural translation  
âœ… Regex-based Arabizi phonetic correction  
âœ… Rule-based orthographic variation generation

---

## ğŸš€ Features

- Translate English prompt-response pairs into Lebanese Arabizi using GPT-3.5/4
- Correct common phonetic inconsistencies with regex
- Generate 2â€“3 orthographic variants per sentence to simulate real-world spelling diversity
- Outputs a clean JSON dataset ready for training NLP models


## ğŸ“‚ Project Structure

```

arabizi_dataset_generator/
â”‚
â”œâ”€â”€ data/                        
â”‚   â”œâ”€â”€ raw/                         #  Original and Input English dataset (CSV/JSON)
â”‚   â”œâ”€â”€ translated/                 # processed datasets / GPT-translated Arabizi files
â”‚   â”œâ”€â”€ corrected/                  # After regex corrections
â”‚   â””â”€â”€ final/                      # Final output with variants
â”‚
â”œâ”€â”€ scripts/                         # Core processing scripts
â”‚   â”œâ”€â”€ 1_preprocess.py             # Clean and prepare English input
â”‚   â”œâ”€â”€ 2_translate_gpt.py          # Translate to Arabizi via GPT
â”‚   â”œâ”€â”€ 3_postprocess_regex.py      # Regex-based correction
â”‚   â”œâ”€â”€ 4_generate_variants.py      # Orthographic variation generation
â”‚   â””â”€â”€ 5_save_output.py            # Save final output to JSON
â”‚
â”œâ”€â”€ utils/                           # Helper functions
â”‚   â”œâ”€â”€ gpt_api.py                  # GPT calling function
â”‚   â”œâ”€â”€ regex_rules.py              # Regex correction dictionary
â”‚   â”œâ”€â”€ variant_rules.py            # Variation generation rules
â”‚   â””â”€â”€ io_utils.py                 # File I/O utilities
â”‚
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ regex_rules.json # Regex phonetic correction patterns
â”‚ â””â”€â”€ variant_rules.json # Orthographic transformation rules
â”‚
â”œâ”€â”€ .env                             # Environment variables (e.g., OpenAI key)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ README.md                        # Project overview and setup
â””â”€â”€ main.py                          # Entrypoint script to run full pipeline
````

---

## ğŸ§ª Input Format

```json
[
  {
    "prompt": "How are you?",
    "response": "I'm fine, thanks."
  }
]
````

---

## âœ… Output Format

```json
[
  {
    "prompt_en": "How are you?",
    "response_en": "I'm fine, thanks.",
    "prompt_arabizi": "kifak?",
    "response_arabizi": "mnih, merci",
    "prompt_variants": ["keefak?", "kefak?", "kifak?"],
    "response_variants": ["mnee7, masi", "mnih, merci", "mnih, mercee"]
  }
]
```

---

## ğŸ“¦ Setup & Usage

1. Clone the repo:

```bash
git clone https://github.com/yourusername/arabizi-synthetic-dataset-generator.git
cd arabizi-synthetic-dataset-generator
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Add your English data in `data/input_dataset.json`

4. Run the generator:

```bash
python main.py
```

---

## ğŸ” OpenAI API Key

Set your OpenAI API key in `gpt_wrapper.py` or via environment variable:

```bash
export OPENAI_API_KEY="your-key-here"
```

---

## âœï¸ Author

**Muhammad Fakhar ul Hasnain**
 |  AI Engineer



